from groq import Groq
from dotenv import load_dotenv
import os


from utils.utils import FixedChatHistory
from utils.utils import chat_completion, build_prompt_structure
from utils.utils import build_chat_history
 


load_dotenv()
GENERATION_SYSTEM_PROMPT = """ Your task is to generate a precise and the best content for the use . 
If the user provied critique , respond with a revised version on you previous response.
You must always output your revised answer .
"""

REFLECTION_SYSTEM_PROMPT = """Your task is to reflect on the content generated by the user.
If the user content has something to imporve or something wrong output a list of recommendations and critique  to improve the content.
If the user content is ok and good and there is nothing to immprove or change just respond with this : <OK>.

"""

class ReflectionAgent:

    def __init__(self, model : str = "llama-3.3-70b-versatile"):
        self.client = Groq()
        self.model=model

    
    def generate(self,generation_chat_history, ):
        respone = chat_completion(
            self.client,
            generation_chat_history,
            model=self.model
        )
        

        return respone

    def reflect(self,reflection_chat_history, ):
        response = chat_completion(
            self.client,
            reflection_chat_history,
            model=self.model
        )

        return response
        
    def run(self, user_message: str,generation_prompt : str, reflection_prompt : str,n_steps : int = 5):
        generation_prompt+= GENERATION_SYSTEM_PROMPT
        reflection_prompt+= REFLECTION_SYSTEM_PROMPT
 
        generation_chat_history = FixedChatHistory([
            build_prompt_structure(generation_prompt, "system"),
            build_prompt_structure(user_message, "user")],
            total_length=3
        )


        reflection_chat_history = FixedChatHistory([
            build_prompt_structure(reflection_prompt, "system")],
            total_length=3
        )

        for i in range(n_steps):

            generation=self.generate(generation_chat_history)


            build_chat_history(
                generation_chat_history,
                generation,
                "assistant"
            )       

            build_chat_history(
                reflection_chat_history,
                generation,
                "user"
            )
            critique = self.reflect(reflection_chat_history)

            if critique == "<OK>":
                break
        


            build_chat_history(
                reflection_chat_history,
                critique,
                "assistant"
            )

            build_chat_history(
                generation_chat_history,
                critique,
                "user"
            )




        return generation









